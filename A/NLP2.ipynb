{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vorarephilia', 'alaskan pipeline', 'assmunch', 'rimjob', 'doggy style', 'goregasm', 'topless', 'creampie', 'tits', 'sadism']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load original data twitter data\n",
    "data_path = r'F:\\code\\NLP\\labeled_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Load dirty word dataset\n",
    "profanity_path =  r'F:\\code\\NLP\\en.txt'\n",
    "with open(profanity_path, 'r') as file:\n",
    "    profanity_list = set([line.strip().lower() for line in file if line.strip()])\n",
    "\n",
    "# check some of the dirty word\n",
    "print(list(profanity_list)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\condaenv\\envs\\NLP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('what', 0), ('the', 0), ('fuck', 1), ('did', 0), ('you', 0), ('bitch', 1), ('say', 0), (',', 0), ('stupid', 0), ('?', 0)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import random_split\n",
    "import re\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # remove spave from begining and ending\n",
    "    return text.lower().strip()\n",
    "\n",
    "\n",
    "# label dirty word funtion\n",
    "def label_text(text, tokenizer, profanity_list):\n",
    "   # lable dirty words as 1, others are 0\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    tokens = tokenizer.tokenize(cleaned_text)\n",
    "    labels = [1 if token in profanity_list else 0 for token in tokens]\n",
    "    return tokens, labels\n",
    "\n",
    "class ProfanityDataset(Dataset):\n",
    "    #using previous rules labels the data\n",
    "    def __init__(self, texts, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = [label_text(text, tokenizer, profanity_list)[1] for text in texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        tokens, labels = label_text(text, tokenizer, profanity_list)\n",
    "        encoding = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_len)\n",
    "        labels += [0] * (self.max_len - len(labels))\n",
    "        labels = torch.tensor(labels[:self.max_len])\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': labels\n",
    "        }\n",
    "# test lablingling \n",
    "example_text = \"What the fuck did you bitch say, stupid?\" # change this sentance if other sentance needs to be tested\n",
    "tokens, labels = label_text(example_text, tokenizer, profanity_list)\n",
    "print(list(zip(tokens, labels)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([ 101, 2054, 1996, 6616, 2106, 2017, 7743, 2360, 1010, 5236, 1029,  102,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "#tesitng the exaample text  with encoding and tokenizer\n",
    "texts = [example_text]  \n",
    "dataset = ProfanityDataset(texts, tokenizer, max_len=128)\n",
    "\n",
    "# encoding result\n",
    "sample_encoding = dataset[0]\n",
    "print(\"Input IDs:\", sample_encoding['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, labels):\n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probabilities = torch.softmax(outputs, dim=-1)\n",
    "    # Get the most likely label (class 1 if binary classification)\n",
    "    predictions = torch.argmax(probabilities, dim=-1)\n",
    "    # Calculate how many predictions match the labels\n",
    "    correct = (predictions == labels).float()\n",
    "    # Calculate the accuracy across all predictions in the batch\n",
    "    accuracy = correct.sum() / correct.numel()\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "f:\\condaenv\\envs\\NLP\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Training: 100%|██████████| 2479/2479 [02:41<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Training loss: 0.0017763516721725076, Average Training Accuracy: 0.9995430365066559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 620/620 [00:12<00:00, 50.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 9.308759511279806e-05, Validation Accuracy: 0.9999763734879032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2479/2479 [02:39<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Training loss: 0.00014375989620913463, Average Training Accuracy: 0.9999637580677693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 620/620 [00:12<00:00, 51.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.374877851577806e-05, Validation Accuracy: 0.9999889742943548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2479/2479 [02:39<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Training loss: 0.0001909132265669591, Average Training Accuracy: 0.9999511521782977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 620/620 [00:12<00:00, 50.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.878670471927748e-05, Validation Accuracy: 0.9999889742943548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2479/2479 [02:40<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Training loss: 3.3338871491504916e-05, Average Training Accuracy: 0.9999944849233562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 620/620 [00:12<00:00, 50.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.00016035336680932146, Validation Accuracy: 0.9999716481854839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2479/2479 [02:43<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Training loss: 6.410279941017107e-05, Average Training Accuracy: 0.9999862123083905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 620/620 [00:12<00:00, 50.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.852588977567217e-05, Validation Accuracy: 0.9999952746975806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2479/2479 [02:43<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Training loss: 3.9311780625496824e-05, Average Training Accuracy: 0.9999909395169423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 620/620 [00:12<00:00, 51.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 3.199396562837731e-05, Validation Accuracy: 0.9999952746975806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 842/2479 [00:54<01:44, 15.71it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from transformers import BertForTokenClassification, AdamW, get_scheduler\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# GPU avaliable check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model chosing\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# prepare the data and pre process\n",
    "dataset = ProfanityDataset(data['tweet'], tokenizer)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# optimizer and learning rate setting \n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "# training \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        accuracy = compute_accuracy(outputs.logits, batch['labels'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        #loss calculation\n",
    "        total_loss += loss.item()\n",
    "        total_accuracy += accuracy.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_train_accuracy = total_accuracy / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(avg_train_accuracy)\n",
    "    print(f\"Epoch {epoch+1}, Average Training loss: {avg_train_loss}, Average Training Accuracy: {avg_train_accuracy}\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    total_eval_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accuracy = compute_accuracy(outputs.logits, batch['labels'])\n",
    "\n",
    "            total_eval_loss += loss.item()\n",
    "            total_eval_accuracy += accuracy.item()\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(val_loader)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(avg_val_accuracy)\n",
    "    print(f\"Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy}\")\n",
    "\n",
    "    # save the best model for Masked model input\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "# plot of loss \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot of accuracy \n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]: not dirty\n",
      "what: not dirty\n",
      "the: Dirty\n",
      "fuck: not dirty\n",
      "did: not dirty\n",
      "you: Dirty\n",
      "bitch: not dirty\n",
      "say: not dirty\n",
      ",: not dirty\n",
      "stupid: not dirty\n",
      "?: not dirty\n",
      "[SEP]: not dirty\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# tesing\n",
    "sentence = example_text\n",
    "\n",
    "# preprocessing the test sentance\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "# Masked LM prediction\n",
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# get the highest preditoin mark\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# get the predicted word\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze().tolist())\n",
    "predicted_labels = predictions.squeeze().tolist()\n",
    "\n",
    "# print the word and labels\n",
    "for token, label in zip(tokens, predicted_labels):\n",
    "    print(f\"{token}: {'Dirty' if label == 1 else 'not dirty'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: What the fuck did you bitch say, stupid?\n",
      "Modified: what the fuck did that bitch say , stupid ?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertForMaskedLM\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#load BERT for masekd LM\n",
    "model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# check if GPU available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# load classification to GPU\n",
    "model.to(device)\n",
    "# load masked to GPU\n",
    "model_mlm.to(device)\n",
    "\n",
    "# example sentance\n",
    "sentence = example_text\n",
    "\n",
    "# using classification modle detect dirty words\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\", add_special_tokens=True)\n",
    "input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "model.eval()\n",
    "model_mlm.eval()\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(['[CLS]'] + tokens + ['[SEP]']) \n",
    "input_tensor = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "outputs = model(input_tensor)\n",
    "predictions = torch.argmax(outputs.logits, dim=-1).squeeze()\n",
    "\n",
    "sensitive_indices = (predictions[1:-1] == 1).nonzero(as_tuple=True)[0].tolist() \n",
    "\n",
    "modified_tokens = ['[CLS]'] + tokens + ['[SEP]']  \n",
    "\n",
    "for idx in sensitive_indices:\n",
    "    idx += 1  \n",
    "    modified_tokens[idx] = '[MASK]'\n",
    "\n",
    "    masked_input_ids = tokenizer.convert_tokens_to_ids(modified_tokens)\n",
    "    masked_input_tensor = torch.tensor([masked_input_ids]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs_mlm = model_mlm(masked_input_tensor)\n",
    "        predicted_index = torch.argmax(outputs_mlm.logits[0, idx], dim=-1).item()\n",
    "        predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "\n",
    "    modified_tokens[idx] = predicted_token \n",
    "\n",
    "modified_tokens = modified_tokens[1:-1] \n",
    "new_sentence = tokenizer.convert_tokens_to_string(modified_tokens)\n",
    "print(\"Original:\", sentence)\n",
    "print(\"Modified:\", new_sentence)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
